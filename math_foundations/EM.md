# EM-инициализация параметров HMM

## 1. Теоретические основы
Алгоритм Expectation-Maximization (EM) для HMM состоит из двух этапов:
- **E-step** (Expectation): Вычисление ожидаемых значений скрытых переменных
- **M-step** (Maximization): Обновление параметров модели

## 2. Формальная постановка задачи
### Входные данные:
- Наблюдения `O = (o₁, ..., o_T)`
- Количество состояний `N`
- Количество возможных наблюдений `M`
- Критерий остановки `ε = 10⁻⁶`

### Искомые параметры:
`λ = (A, B, π)`, где:
- `A = {aᵢⱼ}` - матрица переходов между состояниями
- `B = {bᵢ(k)}` - матрица вероятностей наблюдений
- `π = {πᵢ}` - начальное распределение вероятностей

## 3. Алгоритм EM

### Шаг 0: Инициализация параметров
```python
# Псевдокод инициализации
def initialize_parameters(N, M):
    A = np.random.rand(N, N)
    A = A / A.sum(axis=1, keepdims=True)  # Нормализация
    
    B = np.random.rand(N, M)
    B = B / B.sum(axis=1, keepdims=True)
    
    π = np.random.rand(N)
    π = π / π.sum()
    
    return A, B, π

```
### Шаг 1: E-step (Forward-Backward алгоритм)

Прямые переменные (α):

`α₁(j) = π_j·b_j(o₁)`

`αₜ(j) = b_j(oₜ)·∑_{i=1}^N α_{t-1}(i)·a_{ij} \quad \text{для } t=2..T`

Обратные переменные (β):

`β_T(i) = 1`

`βₜ(i) = ∑_{j=1}^N a_{ij}·b_j(o_{t+1})·β_{t+1}(j) \quad \text{для } t=T-1..1`

Вспомогательные величины:

`γₜ(j) = P(xₜ = s_j | O, λ) = \frac{αₜ(j)βₜ(j)}{∑_{j=1}^N αₜ(j)βₜ(j)}`

`ξₜ(i,j) = P(x_{t-1}=s_i, xₜ=s_j | O, λ) = \frac{α_{t-1}(i)a_{ij}b_j(oₜ)βₜ(j)}{∑_{i,j} α_{t-1}(i)a_{ij}b_j(oₜ)βₜ(j)}`